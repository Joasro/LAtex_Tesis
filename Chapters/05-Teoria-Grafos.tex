\chapter{Optimización de la Oferta Académica mediante IA}
\label{cp:Capitulo6}

\insertminitoc
\parindent0pt

\section{Marco Conceptual del Sistema Propuesto}
El marco conceptual del sistema propuesto se fundamentó en la integración sistémica de tecnologías de información para la gestión académica superior. Se definió una arquitectura holística donde los datos históricos institucionales interactuaron con algoritmos de inferencia, estructurando un ecosistema avanzado de soporte a la decisión. Se estableció de forma categórica que este modelo computacional no pretendió automatizar la toma de decisiones definitivas, sino dotar a las unidades de jefatura de una plataforma analítica robusta. De este modo, la incertidumbre inherente a la predicción de la demanda estudiantil se transformó en escenarios operativos viables, garantizando una distribución de recursos fundamentada en evidencia cuantitativa y avalada por el criterio experto humano.

Para cimentar esta arquitectura a nivel institucional, se adoptó el paradigma del campus inteligente multidimensional. Silva-da-Nóbrega et al. \cite{silva-da-nobrega_smart_2022} propusieron un marco de trabajo de \textit{Smart Campus} estructurado de manera integral para abordar los Objetivos de Desarrollo Sostenible (ODS), identificando la ``Gestión Inteligente'' y la ``Gobernanza Inteligente'' como dimensiones indispensables. En el diseño conceptual del sistema, se incorporaron estas dimensiones para asegurar que la optimización de la oferta académica no solo respondiera a requerimientos numéricos de capacidad física, sino que se alineara con estrategias de sostenibilidad e innovación universitaria. Se determinó que concebir el entorno académico como un ecosistema digital interconectado facilitó la interoperabilidad requerida entre los módulos de recolección de registros y los motores de inteligencia artificial.

Asimismo, se integró la dimensión humana como un componente crítico dentro de la operatividad del marco. Zhou y Schofield \cite{zhou_developing_2024} desarrollaron un modelo conceptual riguroso para la alfabetización en Inteligencia Artificial en la educación superior, dividiendo las competencias en dimensiones cognitivas y socioemocionales. Se aplicó este principio al definir la interacción de la jefatura de carrera con el sistema, estableciendo que la efectividad de la herramienta requirió que el usuario comprendiera los fundamentos de la lógica algorítmica y los contornos éticos de las predicciones. Se estructuró la dinámica del sistema para requerir esta alfabetización, garantizando que el tomador de decisiones abordara las recomendaciones algorítmicas desde un enfoque analítico, reconociendo posibles sesgos antes de ejecutar la programación final de secciones.

Esta interacción transparente humano-máquina habilitó la evolución estratégica de la unidad académica. George y Wooden \cite{george_managing_2023} analizaron exhaustivamente la metamorfosis de las universidades tradicionales hacia ecosistemas inteligentes, enfatizando que la aplicación de la IA en procesos administrativos trasciende la mera eficiencia técnica para consolidarse como un activo estratégico fundamental. Se definió que el sistema propuesto operó como el catalizador directo de esta transformación, permitiendo que la planificación de la oferta curricular evolucionara de una práctica administrativa reactiva —sostenida en la inercia de promedios históricos— a un modelo organizacional anticipatorio. La investigación corroboró que la inserción de estas tecnologías redefinió la capacidad operativa de la institución, optimizando la asignación de capital intelectual y de infraestructura frente a variables complejas.

En el nivel operativo, el marco conceptual internalizó los beneficios pragmáticos de la digitalización de procesos. Yang \cite{yang_exploration_2024} evaluó la ruta de transformación digital específica para la gestión educativa superior, demostrando mediante evidencia cuantitativa que la reingeniería de procesos basada en analítica de datos mitigó drásticamente la saturación administrativa. En la parametrización de este proyecto, se adoptó dicha ruta metodológica para garantizar que el flujo de información curricular entre los departamentos operara sin interrupciones, eliminando cuellos de botella en la consolidación de horarios. Se observó que la estructuración digital de estos procedimientos no solo aceleró la operatividad logística, sino que mejoró los indicadores de calidad y la trazabilidad requerida en auditorías académicas.

Para viabilizar el flujo continuo y seguro de información hacia los motores predictivos, se definió una capa subyacente de integración tecnológica automatizada. Ravi \cite{ravi_etl_2025} exploró la evolución ingenieril de la automatización en los flujos de Extracción, Transformación y Carga (ETL), argumentando que las arquitecturas contemporáneas exigen procesamiento en tiempo real o cuasi-real para no perder valor analítico. Se estructuró el componente de ingestión de datos del sistema propuesto bajo este rigor técnico, prescribiendo tuberías (pipelines) autónomas que extrajeron los registros fragmentados, los depuraron de inconsistencias y los disponibilizaron en formatos matriciales sin requerir intervención manual constante. Se estableció que esta automatización constituyó el pilar técnico indispensable para la generación de pronósticos precisos por parte de los clasificadores automáticos.

Finalmente, el marco conceptual propuesto halló su máxima justificación en la convergencia entre la capacidad de ejecución digital y la dirección estratégica de alto nivel. Al contrastar los postulados teóricos de George y Wooden \cite{george_managing_2023} con los hallazgos empíricos de Yang \cite{yang_exploration_2024}, se validó la integridad del modelo de soporte a la decisión. Mientras George y Wooden \cite{george_managing_2023} aportaron el andamiaje filosófico y estructural que elevó a la IA como el habilitador principal de la universidad inteligente, Yang \cite{yang_exploration_2024} corroboró estadísticamente que la aplicación de estas herramientas materializó reducciones tangibles en los tiempos de respuesta organizacional. Se concluyó que la arquitectura del sistema articuló armónicamente ambas perspectivas, dotando a la jefatura de una solución tecnológica que demostró ser innovadora en su concepción estratégica e implacablemente eficiente en su implementación administrativa diaria.

\section{Censo Académico Inteligente: Integración de la Intención de Demanda Estudiantil}
La formulación de una oferta académica pertinente exigió trascender el análisis exclusivo de los registros históricos de matrícula. Se determinó que proyectar la apertura de secciones basándose únicamente en el comportamiento pasado generaba sesgos, ignorando las fluctuaciones inmediatas en los intereses del estudiantado. Para mitigar esta deficiencia, se diseñó e implementó un Censo Académico Inteligente, concebido como un instrumento dinámico de recolección de datos preventrícula. Este mecanismo permitió capturar la ``intención de demanda'' declarada por los estudiantes, transformando una variable cualitativa e incierta en un vector de datos estructurado y auditable, el cual sirvió como insumo primario para calibrar los motores de predicción antes de consolidar la programación oficial.

Para la captura y estructuración de esta información prospectiva, se optimizó la arquitectura del Sistema de Gestión de Información (IMS) de la institución. Zhang et al. \cite{zhang_data_2022} exploraron las aplicaciones de la minería de datos en el desarrollo de los IMS universitarios, estableciendo que la acumulación pasiva de datos estudiantiles carecía de valor sin algoritmos de extracción. Se integró una lógica de procesamiento directamente en la base de datos del censo, aplicando técnicas de asociación para vincular las respuestas de la encuesta con el expediente real del alumno. El estudio demostró que anclar la minería de datos en el núcleo del IMS permitió descubrir patrones de comportamiento ocultos, validando que el censo no operara como un formulario aislado, sino como un módulo interactivo del ecosistema de información académica.

Se estableció que la intención declarada por el estudiante debía ponderarse algorítmicamente contra su perfil demográfico y académico para estimar la probabilidad real de inscripción. Shao et al. \cite{shao_machine_2022} aplicaron métodos de aprendizaje automático para la predicción de matrícula de cursos, identificando que ciertas variables poseían un peso predictivo superior. Mediante el uso de métricas de ``importancia de variables'' (Variable Importance) derivadas de algoritmos de Random Forest, se comprobó que el nivel de clase del estudiante, el periodo de ingreso y la fecha de cumplimiento de sus prerrequisitos eran determinantes críticos. Se aplicó este principio al censo inteligente: el sistema no asumió que cada respuesta afirmativa se traduciría en una matrícula efectiva. Se aplicó este principio al censo inteligente: el sistema no asumió que cada respuesta afirmativa se traduciría en una matrícula efectiva, sino que ponderó la intención censal utilizando el historial académico del encuestado para filtrar falsos positivos.

La fluidez operativa entre la plataforma del censo y el motor de predicción se garantizó mediante una capa de interoperabilidad. Pérez-Jorge et al. \cite{perez-jorge_impact_2025} evaluaron el impacto de las Interfaces de Programación de Aplicaciones (APIs) impulsadas por IA en la gestión de la información educativa. Se implementó una arquitectura de APIs RESTful que permitió la ingesta de los datos censales en tiempo real. La investigación determinó que la fragmentación de sistemas constituyó históricamente el mayor obstáculo para la administración universitaria; por consiguiente, el uso de APIs inteligentes aseguró que cada registro de intención de matrícula alimentara instantáneamente los modelos de clasificación, eliminando la latencia de procesamiento y los errores asociados a la exportación manual de datos.

El cruce metodológico entre la recolección de la intención censal y la viabilidad real de inscripción requirió una validación algorítmica estricta. Al integrar la arquitectura de información propuesta por Zhang et al. \cite{zhang_data_2022} con los modelos predictivos de Shao et al. \cite{shao_machine_2022}, se estableció un mecanismo de depuración de la demanda. Mientras el sistema IMS de Zhang et al. \cite{zhang_data_2022} proveyó la infraestructura para recolectar masivamente las intenciones de los estudiantes mediante reglas de asociación, el modelo de \textit{Random Forest} de Shao et al. \cite{shao_machine_2022} actuó como un filtro probabilístico. Ambos enfoques convergieron para resolver la discrepancia empírica donde los estudiantes solicitan asignaturas para las cuales no cumplen los prerrequisitos; el algoritmo cruzó la intención del censo con el árbol de dependencias, descartando estadísticamente las solicitudes inviables y generando una demanda depurada.

Además de la exactitud matemática, se consideró el impacto del censo en el ecosistema digital del estudiante. Retomando la investigación de Pérez-Jorge et al. \cite{perez-jorge_impact_2025}, se evidenció que la gestión de información mediante APIs no solo optimizó el backend institucional, sino que transformó la experiencia del usuario final. Se diseñó la interfaz del censo para interactuar dinámicamente con el expediente del alumno, mostrando únicamente las asignaturas proyectadas según su avance curricular específico. Se concluyó que esta personalización de la toma de datos incrementó significativamente la tasa de participación en la encuesta, ya que el estudiante percibió el instrumento no como un trámite burocrático genérico, sino como una herramienta de asesoría académica adaptada a su trayectoria.

Finalmente, se validó que la integración del censo inteligente como variable de entrada mitigó los riesgos financieros de la institución. Shao et al. \cite{shao_machine_2022} argumentaron que la exactitud en el pronóstico de las tasas de inscripción es imperativa para minimizar costos administrativos innecesarios y reducir la carga sobre el personal docente y de planificación. Se determinó que el sistema, al preprocesar la intención estudiantil depurada, entregó a la jefatura de carrera un panorama de alta fidelidad sobre los requerimientos reales de cupos. Esta precisión algorítmica evitó la apertura de secciones subutilizadas —con el consecuente ahorro en pago de honorarios— y previno la saturación de aulas, alineando milimétricamente la oferta institucional con la demanda efectiva.

\section{Aplicación de Aprendizaje Automático para el Ajuste de la Cadencia Académica}
El ajuste de la cadencia académica se definió como la sincronización estratégica entre el avance real de las cohortes estudiantiles y la oferta institucional de asignaturas. Se identificó que el principal detonante del rezago estudiantil no radicaba exclusivamente en el rendimiento académico individual, sino en las interrupciones del flujo curricular causadas por una programación de clases desalineada con los prerrequisitos críticos. Para mitigar esta problemática, se aplicaron técnicas de aprendizaje automático orientadas a modelar el plan de estudios de Ingeniería en Sistemas como un ecosistema dinámico. El sistema se diseñó para monitorear la progresión histórica y sugerir aperturas de secciones que garantizaran un tránsito ininterrumpido, permitiendo a la jefatura anticiparse a los ``cuellos de botella'' antes de que afectaran los tiempos de egreso.

Para estructurar lógicamente este flujo, el primer paso consistió en la parametrización algorítmica del plan de estudios. Alshanqiti et al. \cite{alshanqiti_rule-based_2020} desarrollaron un enfoque basado en reglas para automatizar la evaluación del mapeo curricular (\textit{Curriculum Mapping}), demostrando que la gestión manual de estas matrices era altamente propensa a errores y contradicciones. Se aplicó esta metodología para traducir el catálogo de asignaturas y sus dependencias normativas a un formato procesable por máquina. Al auditar las relaciones lógicas mediante algoritmos, el sistema logró identificar ineficiencias estructurales intrínsecas en la malla, proveyendo a los tomadores de decisiones de un diagnóstico preciso sobre qué secuencias de clases debían ofertarse ininterrumpidamente para respetar las normativas de progresión.

Una vez digitalizadas las reglas curriculares, se procedió al análisis matemático del peso de cada asignatura. Retomando el marco analítico de Stavrinides y Zuev \cite{stavrinides_course-prerequisite_2023}, se modeló la carrera universitaria utilizando \acrfull{cpn}, representando el currículo como un \acrfull{dag}. Se determinó, mediante el cálculo de medidas de ``centralidad'' topológica, que ciertas asignaturas actuaban como nodos críticos; es decir, materias cuya no apertura paralizaría a porcentajes mayoritarios de la población estudiantil. Esta topología de red dotó a la inteligencia artificial de la capacidad de priorizar algorítmicamente las recomendaciones de oferta, alertando a la jefatura cuando una materia de alta centralidad presentaba un déficit de secciones proyectadas.

La robustez del motor de ajuste de cadencia se garantizó al fusionar la validación lógica con el análisis estructural. Al integrar el motor basado en reglas de Alshanqiti et al. \cite{alshanqiti_rule-based_2020} con el modelado de redes complejas de Stavrinides y Zuev \cite{stavrinides_course-prerequisite_2023}, se consolidó un sistema de evaluación bidimensional. Mientras Alshanqiti et al. \cite{alshanqiti_rule-based_2020} aportaron el rigor para que las secuencias sugeridas cumplieran irrestrictamente con la normativa académica institucional, Stavrinides y Zuev \cite{stavrinides_course-prerequisite_2023} proporcionaron el peso jerárquico de cada clase dentro de esa red. Se concluyó que esta intersección tecnológica permitió al asistente algorítmico no solo sugerir asignaturas ``legales'' para el estudiante, sino identificar aquellas estratégicamente indispensables para desaturar los niveles intermedios de la carrera.

Para que los algoritmos de ajuste curricular operaran con precisión, fue imperativo alimentar los modelos con datos longitudinales sobre la velocidad de avance de los estudiantes. Basándose en la arquitectura propuesta por Amo et al. \cite{amo_educational_2021}, se utilizó el concepto de Almacén de Datos Educativos (\textit{Educational Warehouse}) para centralizar la trazabilidad histórica de las cohortes. Se extrajeron patrones de comportamiento que revelaron el tiempo real que los estudiantes tomaban para superar los nodos críticos identificados previamente. El análisis profundo de estos registros almacenados comprobó que el flujo teórico del plan de estudios rara vez coincidía con la progresión empírica, proporcionando a la IA el contexto temporal necesario para ajustar la cadencia de la oferta a la realidad operativa del alumnado.

El ajuste de la cadencia académica también contempló la viabilidad humana del estudiante frente a la dificultad de las asignaturas. Bhosale y Hore \cite{prof_ramkrishna_more_college_pradhikaran_pune_india_ai-based_2025} desarrollaron un motor de programación académica basado en inteligencia artificial orientado a la asignación inteligente del tiempo de estudio. Se integró esta lógica heurística al sistema de jefatura para evitar la sugerencia de bloques horarios sobrecargados de materias de alta complejidad matemática o técnica. Se determinó que una oferta académica optimizada no debía incitar a los estudiantes a matricular simultáneamente múltiples asignaturas de alta reprobación, ya que esto inducía al fracaso masivo. El algoritmo ajustó las recomendaciones para promover una distribución equilibrada de la carga cognitiva por periodo académico.

Finalmente, la aplicación integral de estas técnicas de aprendizaje automático sobre la cadencia del plan de estudios demostró un impacto directo en la sostenibilidad del modelo educativo. Retomando la premisa del modelado de redes de Stavrinides y Zuev \cite{stavrinides_course-prerequisite_2023}, se evidenció que sincronizar la apertura de secciones con el ritmo real de la masa estudiantil maximizó la eficiencia terminal de la carrera. Se estableció que el sistema de soporte, al presentar a la jefatura una visualización clara de los estrangulamientos curriculares y sugerir combinaciones equilibradas, funcionó como un mecanismo proactivo de retención. Esta alineación algorítmica transformó los datos estáticos en estrategias de intervención dinámica, asegurando que los recursos docentes se invirtieran precisamente donde generaban el mayor retorno en términos de fluidez académica.

\section{Módulo de Predicción de Cupos y Secciones}

El Módulo de Predicción de Cupos y Secciones se concibió como el componente cuantitativo central del sistema de soporte a la decisión. Una vez analizado el flujo de la cadencia académica y depurada la intención del estudiante mediante el censo, se requirió traducir esa información en valores numéricos exactos. Se determinó que el sistema debía calcular matemáticamente la cantidad proyectada de estudiantes por asignatura para, consecuentemente, sugerir el número óptimo de secciones (paralelos) a habilitar. Este cálculo se diseñó para operar de manera conservadora y precisa, buscando el equilibrio exacto que evitara tanto la saturación de las aulas como el despilfarro de recursos institucionales derivado de la apertura de secciones con baja densidad poblacional.

En la estructuración inicial de este módulo, se adoptó un enfoque analítico sustentado en el comportamiento histórico de la matrícula. Chang \cite{chang_data-driven_2025} examinó la planificación de inscripciones impulsada por datos, demostrando que la predicción de cupos impactó directamente en decisiones políticas institucionales, contrataciones docentes y asignación presupuestaria. Se integraron modelos de pronóstico basados en suavizado exponencial (\textit{Exponential Smoothing}), los cuales permitieron capturar la tendencia evolutiva y la estacionalidad de los datos a lo largo de los periodos académicos. La aplicación de este modelado de series temporales garantizó que las fluctuaciones históricas —como los picos de matrícula en el primer semestre del año— fueran asimiladas por el algoritmo antes de emitir una sugerencia numérica a la jefatura.

Para adaptar estas proyecciones a las dinámicas específicas de las carreras técnicas, se evaluó la eficacia de distintos modelos de regresión. Pauta Riera et al. \cite{riera_pronostico_2025} realizaron un estudio comparativo en una unidad académica de ingeniería, aplicando modelos lineales, logísticos y polinómicos para predecir la demanda estudiantil. La investigación determinó que los modelos de regresión polinómica presentaron un coeficiente de determinación superior, logrando ajustarse con mayor fidelidad a la varianza de los registros universitarios. Se incorporó esta lógica matemática al núcleo del módulo, estableciendo que la predicción de secciones en Ingeniería en Sistemas no seguía un crecimiento lineal simple, sino un comportamiento polinómico afectado por las tasas de reprobación y retención de las ciencias exactas.

Con el objetivo de elevar la precisión y minimizar el error residual de la regresión clásica, se integraron algoritmos avanzados de aprendizaje automático. Shao et al. \cite{shao_machine_2022} implementaron métodos de \textit{Random Forest} y Árboles de Clasificación y Regresión (CART) para pronosticar las tasas de inscripción. Se evidenció que la incorporación de variables demográficas y académicas del estudiante dentro de estos algoritmos incrementó drásticamente la exactitud de la predicción. Se configuró el módulo para que el \textit{Random Forest} procesara múltiples árboles de decisión de forma simultánea, evaluando miles de interacciones entre el avance curricular del estudiante y el censo académico, lo que redujo sustancialmente los costos administrativos asociados a la subestimación de la demanda.

La arquitectura computacional del módulo también contempló el procesamiento de registros académicos semiestructurados que los algoritmos de regresión tradicionales no lograban interpretar. Hamill e Iqbal \cite{hamill_regression-based_2026} propusieron una arquitectura innovadora empleando Modelos de Lenguaje Pequeños (\textit{Small Language Models} - SLM) equipados con un cabezal de regresión (\textit{regression head}) para extraer métricas cuantitativas a partir de datos complejos. Se adoptó este enfoque vanguardista para dotar al sistema de la capacidad de interpretar notas, observaciones y transcripciones irregulares en los expedientes de los estudiantes. Se determinó que el procesamiento semántico previo realizado por el SLM, seguido de una salida numérica acotada, enriqueció los vectores de datos que alimentaron el pronóstico final de cupos.

La fiabilidad suprema del módulo de predicción se logró mediante la hibridación de metodologías analíticas. Al integrar el modelado de tendencias macroscópicas de Chang \cite{chang_data-driven_2025} con la precisión microscópica de los algoritmos descritos por Shao et al. \cite{shao_machine_2022}, se estructuró un motor de inferencia altamente resiliente. Mientras el análisis de series temporales de Chang \cite{chang_data-driven_2025} proporcionó la línea base del crecimiento vegetativo de la institución (la tendencia general de la universidad), el \textit{Random Forest} de Shao et al. \cite{shao_machine_2022} ajustó ese número base analizando las características individuales de cada alumno inscrito. Se concluyó que esta convergencia algorítmica garantizó que la sugerencia final de cupos no ignorara el contexto histórico global ni los matices específicos de la cohorte actual.

Finalmente, el valor numérico resultante del procesamiento de cupos se tradujo en parámetros logísticos accionables. Retomando la problemática expuesta por Pauta Riera et al. \cite{riera_pronostico_2025} sobre la saturación física, el sistema dividió algorítmicamente la predicción total de cupos entre la capacidad pedagógica máxima normada por la institución (por ejemplo, 40 estudiantes por aula). Este cálculo determinó el número exacto de secciones recomendadas a la jefatura de carrera. Se estableció que esta salida cuantitativa sirvió como el insumo directo para el diseño de la carga académica docente, proveyendo al tomador de decisiones de un escenario estructurado que eliminó la improvisación en la apertura de paralelos de última hora.

\section{Estrategias de Optimización de Recursos Físicos y Horarios}
La transición de la predicción numérica de cupos a la materialización de la oferta académica requirió resolver el desafío logístico de la distribución espacio-temporal. Se determinó que asignar horarios y aulas de forma manual para las secciones proyectadas constituía un proceso ineficiente, propenso a colisiones y cognitivamente extenuante para el personal administrativo. Por lo tanto, se diseñaron estrategias de optimización algorítmica orientadas a emparejar la demanda estudiantil validada con la disponibilidad finita de recursos físicos e instructores. Este subsistema operó bajo la premisa de generar combinaciones matriciales complejas, transformando el pronóstico de matrícula en una planificación operativa ejecutable, lista para la revisión de la jefatura.

Para abordar la complejidad matemática de esta asignación, se enmarcó la problemática dentro del Problema de Horarios de Cursos Universitarios (UCTP, por sus siglas en inglés). Abdipoor et al. \cite{abdipoor_meta-heuristic_2023} analizaron este fenómeno, clasificándolo inherentemente como un problema de optimización combinatoria de tipo NP-Hard (\textit{NP-hard}), donde el espacio de soluciones posibles crece exponencialmente con cada nueva asignatura, profesor o aula añadida. Se adoptó este enfoque teórico para justificar la inviabilidad de los métodos de fuerza bruta convencionales en la planificación universitaria, implementando en su lugar aproximaciones metaheurísticas. Se estableció que el uso de estas técnicas avanzadas permitió al sistema explorar ágilmente el vasto espacio de búsqueda y converger hacia una asignación de horarios óptima sin agotar los recursos computacionales.

La operatividad del algoritmo de optimización se condicionó a un sistema estricto de reglas institucionales. Bashab et al. \cite{bashab_optimization_2023} sistematizaron las metodologías en el UCTP dividiendo las normativas en restricciones duras (\textit{hard constraints}) y blandas (\textit{soft constraints}). Se programó el motor de inferencia asimilando esta categorización: se impidió algorítmicamente el solapamiento de docentes en una misma franja horaria o la asignación de grupos que excedieran la capacidad del aula (restricciones duras inquebrantables). Simultáneamente, el algoritmo fue instruido para maximizar el cumplimiento de preferencias de jornada de los profesores y minimizar los lapsos sin clases para los estudiantes (restricciones blandas). La investigación evidenció que modelar el problema bajo esta dicotomía garantizó la viabilidad técnica y el confort humano.

Para la generación final de la cuadrícula, se transicionó hacia un modelo de inteligencia artificial de carácter evolutivo. Farinola y Assogba \cite{farinola_explicit_2025} desarrollaron un generador explícito de horarios mediante IA, demostrando la eficacia de los \acrfull{ga} frente a las heurísticas tradicionales para la resolución de colisiones. Se incorporó esta base algorítmica al subsistema logístico, permitiendo que la máquina evaluara iterativamente miles de configuraciones candidatas (``cromosomas''), aplicando funciones de selección cruzada y mutación hasta hallar la matriz con la menor tasa de penalización. Se constató que la automatización explícita de este proceso liberó a las unidades académicas de prolongadas semanas de labor manual, entregando borradores de horarios robustos en cuestión de minutos.

Más allá de la asignación temporal, se abordó la eficiencia intrínseca del uso del espacio físico. Adaptando los principios de macrodatos expuestos por Himeur et al. \cite{himeur_ai-big_2023} sobre la automatización de sistemas de gestión de edificios \acrfull{bams}, se estructuró una capa de inteligencia espacial. Aunque enfocada originalmente en el rendimiento energético, la lógica analítica de este estudio se extrapoló para integrar las dimensiones arquitectónicas de las aulas con las predicciones del módulo de cupos. Se logró que el sistema no asignara auditorios con capacidad para cincuenta personas a secciones técnicas con una demanda proyectada de quince estudiantes, optimizando la huella de ocupación y asegurando un uso racional del patrimonio inmobiliario de la facultad.

La consolidación de la malla horaria óptima representó el éxito de integrar estructuras punitivas con procesos generativos. Al entrelazar la taxonomía de restricciones de Bashab et al. \cite{bashab_optimization_2023} con el motor de optimización evolutiva de Farinola y Assogba \cite{farinola_explicit_2025}, se construyó una arquitectura híbrida inquebrantable. Mientras Bashab et al. \cite{bashab_optimization_2023} proveyeron el modelo matemático de penalizaciones (definiendo algorítmicamente el costo de cada restricción vulnerada), Farinola y Assogba \cite{farinola_explicit_2025} suministraron el mecanismo de los Algoritmos Genéticos para evadir dichos choques iterativamente y reducir el puntaje de penalización a cero. Se concluyó que esta interdependencia tecnológica fue el factor clave que permitió al sistema resolver el desafío combinatorio sin entrar en bucles de procesamiento infinitos.

Finalmente, se estableció que el resultado de estas optimizaciones espaciotemporales operó estrictamente bajo el paradigma de soporte a la decisión. Se determinó que el algoritmo generador de horarios no se ejecutaría de forma autónoma con publicación directa en los portales estudiantiles, sino que presentaría sus configuraciones en formato de ``propuestas operativas'' ante la jefatura de carrera. Esta estrategia arquitectónica preservó la jerarquía del coordinador académico, quien retuvo la capacidad absoluta de realizar ajustes manuales para atender contingencias no cuantificables algorítmicamente, tales como licencias médicas imprevistas del profesorado. De esta forma, el sistema aportó la potencia computacional para resolver el entramado multidimensional, respetando invariablemente la flexibilidad inherente a la administración educativa.

\section{Sistemas de Apoyo a la Toma de Decisiones para Unidades de Jefatura}
El diseño de la interfaz entre los algoritmos matemáticos y las autoridades académicas se materializó a través de un Sistema de Apoyo a la Toma de Decisiones (DSS). Se estableció que la complejidad de los modelos predictivos y de optimización carecía de valor práctico si sus resultados no se presentaban de manera comprensible para los tomadores de decisiones no técnicos. Por consiguiente, el DSS se configuró no como una herramienta de ejecución automática, sino como un tablero de control consultivo (\textit{dashboard}). Este entorno digital permitió a la jefatura de carrera visualizar las sugerencias de apertura de secciones, evaluar diferentes escenarios simulados y aplicar su juicio experto antes de oficializar la oferta académica, manteniendo invariablemente la autoridad humana sobre la planificación final.

Para estructurar la evaluación de las propuestas algorítmicas, se implementó un enfoque de análisis multidimensional. Bas et al. \cite{bas_multi-criteria_2025} desarrollaron un sistema de soporte de decisiones multicriterio para evaluar la efectividad de los cursos, demostrando que las decisiones educativas no pueden sustentarse en una única variable métrica. Se adaptó esta metodología para que la jefatura evaluara la apertura de secciones ponderando simultáneamente diversos factores: la demanda estudiantil predicha, la disponibilidad presupuestaria, el índice de reprobación histórico y la carga horaria docente. La investigación validó que estructurar el DSS bajo este paradigma multicriterio impidió que el algoritmo sobreoptimizara una sola variable (por ejemplo, reducir costos) a expensas de la calidad académica.

La adopción del sistema por parte de la jefatura requirió mitigar la opacidad inherente a los modelos de aprendizaje automático complejos, coloquialmente conocidos como ``cajas negras''. Almtrf \cite{almtrf_integrating_2025} demostró que la integración de \acrfull{xai} en los DSS es fundamental para promover la transparencia y la confianza en la toma de decisiones gerenciales. Se dotó al sistema de técnicas de explicabilidad algorítmica, como \acrfull{lime} y \acrfull{shap}, las cuales acompañaron cada sugerencia de sección con un desglose visual de los factores que la motivaron. Se comprobó que proporcionar esta justificación matemática explícita empoderó a los directivos académicos, permitiéndoles defender sus decisiones de planificación ante los órganos de gobierno institucional con evidencia rastreable.

Previo a la institucionalización de estas herramientas de visualización, se definieron directrices estrictas de gobernanza sobre el uso de la IA. Kaşarcı et al. \cite{kasarci_managing_2025} ejecutaron una revisión sistemática sobre la ética de la IA en la educación superior, advirtiendo que las respuestas institucionales suelen ser reactivas ante el sesgo algorítmico y los riesgos de privacidad. Atendiendo a estas advertencias, el DSS se programó con alertas de sesgo preventivas; por ejemplo, notificando a la jefatura si una programación sugerida favorecía desproporcionadamente a los estudiantes de los últimos semestres en detrimento de los de primer ingreso. Se determinó que incrustar estas políticas éticas directamente en la interfaz gráfica garantizó una adopción tecnológica alineada con los valores institucionales.

El impacto de las decisiones soportadas por el sistema se extendió orgánicamente hacia la salvaguarda de la equidad educativa. Shaikhanova et al. \cite{shaikhanova_interpretable_2025} diseñaron un DSS consciente de la carga de trabajo mediante modelos predictivos interpretables, con el propósito de identificar tempranamente a estudiantes en riesgo sin incurrir en discriminación algorítmica. Se asimiló este paradigma para que la jefatura pudiera evaluar cómo la distribución de horarios afectaba a distintas poblaciones estudiantiles, como aquellos que laboran durante la jornada diurna. La investigación validó que un DSS diseñado bajo los principios de equidad permitió a la administración universitaria configurar una oferta de asignaturas verdaderamente inclusiva, minimizando las barreras estructurales para la culminación de la carrera.

La consolidación de un entorno de decisión justo y transparente se logró al cruzar los preceptos de gobernanza ética con la interpretabilidad algorítmica técnica. Al integrar el marco de gobernanza de Kaşarcı et al. \cite{kasarci_managing_2025} con la arquitectura de equidad de Shaikhanova et al. \cite{shaikhanova_interpretable_2025}, se resolvió la tensión histórica entre la eficiencia automatizada y la responsabilidad humana. Mientras Kaşarcı et al. \cite{kasarci_managing_2025} establecieron el requisito normativo de evitar el sesgo en las instituciones de educación superior, Shaikhanova et al. \cite{shaikhanova_interpretable_2025} proporcionaron la estructura técnica interpretable (XAI) para auditar dicho sesgo en tiempo real. Se concluyó que esta interdependencia aseguró que el Jefe de Carrera no actuara como un simple espectador de los dictámenes de la máquina, sino como un auditor crítico dotado de las herramientas visuales necesarias para garantizar que la programación académica fuera técnica y moralmente inobjetable.

Finalmente, se comprobó que la inserción de este entorno analítico transfiguró radicalmente las funciones operativas de la unidad académica. Retomando la investigación empírica de Almtrf \cite{almtrf_integrating_2025} sobre la confianza en los DSS, se evidenció que la automatización de la complejidad matemática liberó un volumen significativo de horas de labor gerencial. Se determinó que este excedente de tiempo permitió al Jefe de Carrera desplazar su enfoque desde la resolución de conflictos logísticos básicos hacia la planificación estratégica a largo plazo y la atención cualitativa del currículo. Así, el sistema de apoyo cumplió su propósito superior: no reemplazó al administrador, sino que lo dotó de capacidades analíticas aumentadas para dirigir el crecimiento sostenible de la facultad.

\section{Sinergia entre la Predicción de Datos y la Planificación Estratégica}
La culminación del modelo de inteligencia artificial no residió únicamente en su capacidad de procesamiento matemático, sino en la sinergia generada entre la predicción de datos y la planificación estratégica de la institución. Se determinó que los pronósticos de matrícula y la optimización de horarios carecían de impacto sostenido si operaban como procesos aislados de la visión gerencial de la unidad académica. Por consiguiente, se consolidó un ecosistema donde la salida algorítmica actuó como el insumo basal para la toma de decisiones directivas a largo plazo. Esta convergencia transformó los datos operativos diarios en inteligencia institucional, permitiendo a la jefatura de carrera abandonar la gestión reactiva de crisis y adoptar una postura anticipatoria que garantizó el uso eficiente de los recursos y el cumplimiento de los objetivos de calidad educativa.

Para que esta sinergia fuera efectiva, el sistema debió asimilar la naturaleza cambiante de la población estudiantil. García Macías et al. \cite{garcia_macias_aprendizaje_2025} analizaron el impacto del aprendizaje adaptativo impulsado por inteligencia artificial en la educación superior, demostrando que la tecnología posee la capacidad de moldearse dinámicamente al perfil del usuario. Se extrapoló este principio pedagógico hacia la gestión administrativa, estableciendo que la oferta académica generada por el sistema no constituyó una matriz estática, sino una planificación adaptativa. La investigación validó que, al ajustar iterativamente las proyecciones de cupos basándose en el rendimiento continuo de las cohortes, la jefatura logró alinear la estrategia de contratación docente y apertura de secciones con las necesidades cognitivas y logísticas reales del momento.

La materialización de estas estrategias dependió directamente de la robustez de la infraestructura de soporte a la decisión. Shwedeh \cite{shwedeh_integration_2024} investigó la integración de la IA en los Sistemas de Soporte a la Decisión (DSS) universitarios utilizando el Modelado de Ecuaciones Estructurales (SEM). Se comprobó cuantitativamente que factores como la calidad de los datos y la preparación organizacional influyeron de manera crítica en la eficacia de las decisiones gerenciales. En el contexto del sistema desarrollado, se aplicó este hallazgo garantizando que los pipelines de datos (ETL) suministraran información depurada al DSS. Se estableció que una alta calidad en los datos de entrada fue el requisito ineludible para que la jefatura confiara en el sistema y, consecuentemente, ejecutara planes estratégicos audaces con un margen de riesgo minimizado.

En el nivel macroinstitucional, el impacto de estas predicciones redefinió la gobernanza administrativa. Almaghrabi et al. \cite{almaghrabi_sok_2024} sistematizaron el conocimiento sobre el impacto de la Minería de Datos Educativos (EDM) en la administración organizacional, distinguiéndola de su uso puramente pedagógico. Se evidenció que la EDM actuó como un habilitador para la formulación de políticas institucionales y la asignación presupuestaria. Se integró este enfoque estratégico al modelo, demostrando que los patrones descubiertos por la IA (como las tasas crónicas de deserción en materias filtro) no solo sirvieron para acomodar horarios, sino que otorgaron a las autoridades evidencia empírica irrefutable para justificar rediseños curriculares o solicitar ampliaciones de presupuesto operativo ante los entes gubernamentales.

La verdadera sinergia estratégica se cristalizó al fusionar la preparación del capital humano con la potencia de la minería de datos. Al contrastar las perspectivas de Shwedeh \cite{shwedeh_integration_2024} y Almaghrabi et al. \cite{almaghrabi_sok_2024}, se definió el marco de éxito para la implementación de la solución. Mientras Shwedeh \cite{shwedeh_integration_2024} demostró que la preparación organizacional (la disposición al cambio cultural de los directivos) fue indispensable para la adopción tecnológica, Almaghrabi et al. \cite{almaghrabi_sok_2024} proveyeron el vehículo analítico (EDM) que transformó esa disposición en resultados administrativos medibles. Ambos estudios coincidieron en que el software por sí solo no genera optimización; se concluyó que la planificación estratégica óptima resultó de la interacción entre algoritmos que procesaron eficientemente grandes volúmenes de datos y líderes académicos capacitados para interpretar y ejecutar políticas basadas en dichas predicciones.

Como resultado de esta integración, el proceso de programación académica evolucionó hacia un ciclo de mejora continua. Retomando los principios de adaptabilidad expuestos por García Macías et al. \cite{garcia_macias_aprendizaje_2025}, se estructuró un mecanismo de retroalimentación donde el sistema aprendió de las decisiones pasadas de la jefatura y de los resultados reales de inscripción. Se determinó que cada periodo académico finalizado enriqueció la base de conocimientos del algoritmo, reduciendo progresivamente el margen de error de las proyecciones futuras. Esta sostenibilidad operativa aseguró que la inversión en infraestructura de inteligencia artificial generara un retorno incremental, consolidando a la carrera de Ingeniería en Sistemas como un referente de gestión administrativa impulsada por datos.

En conclusión, el desarrollo exhaustivo del marco teórico y conceptual cimentó las bases científicas, tecnológicas y gerenciales de la investigación. A través de la revisión del problema de asignación de recursos, el diseño de arquitecturas de Big Data y el despliegue de modelos de aprendizaje automático y sistemas de soporte, se fundamentó teóricamente la viabilidad de la optimización de la oferta académica. La sinergia lograda entre la predicción cuantitativa y la planificación estratégica demostró que la inteligencia artificial puede integrarse como un asesor ético e implacable frente a la complejidad universitaria. Con el modelo conceptual robustamente justificado, la investigación procedió hacia la fase metodológica y de implementación experimental para validar empíricamente estas premisas teóricas.