\chapter{Inteligencia Artificial y Gestión de Datos}
\label{cp:Capitulo5}

\insertminitoc
\parindent0pt

\section{Inteligencia Artificial aplicada a la gestión administrativa}
La gestión administrativa en las instituciones de educación superior experimentó una reconfiguración fundamental, transitando de modelos operativos tradicionales a ecosistemas digitales de apoyo a la decisión. Se identificó que la mera digitalización de documentos resultó insuficiente para responder a la complejidad del entorno actual; por tanto, se requirió el diseño de sistemas capaces de procesar información y proponer escenarios estratégicos. En este nuevo paradigma, la Inteligencia Artificial (IA) se posicionó no como un ente autónomo de ejecución, sino como una herramienta de consultoría avanzada (\textit{Decision Support System}) que permitió a los gestores anticipar demandas y evaluar opciones de distribución de recursos con una precisión inalcanzable mediante métodos manuales.

El impulso hacia esta transformación tecnológica se vinculó directamente con crisis globales que expusieron la fragilidad de los sistemas analógicos. García-Morales et al. \cite{garcia-morales_transformation_2021} examinaron la transformación de la educación superior tras la disrupción del COVID-19, determinando que las instituciones se vieron forzadas a realizar una transición abrupta hacia entornos digitales. Se observó que este cambio no planificado reveló carencias críticas en la capacidad de respuesta administrativa. El estudio concluyó que la sostenibilidad universitaria dependió de transformar estos mecanismos de emergencia en estrategias digitales integrales, donde la tecnología sirvió para estructurar la información caótica y presentarla de manera coherente para la toma de decisiones humanas.

En respuesta a esta necesidad de evolución, se consolidó el modelo de gestión estratégica asistida por datos. George y Wooden \cite{george_managing_2023} analizaron la adopción de la IA bajo el marco de las ``Universidades Inteligentes'' (\textit{Smart Universities}). Se estableció que la integración de tecnologías avanzadas permitió remodelar los procesos administrativos, moviendo a la institución de una postura reactiva a una predictiva. Los autores argumentaron que la gestión moderna implicó el uso de algoritmos para sugerir escenarios de personalización y asignación de activos, dotando a los directivos de un ``tablero de control'' inteligente que facilitó la elección de la estrategia más adecuada frente a la incertidumbre del mercado educativo.

La aplicación práctica de estas tecnologías demostró beneficios tangibles en la operatividad diaria como herramienta de asistencia. Cisneros Zumba et al. \cite{cisneros_zumba_uso_2025} evaluaron el uso de la IA en la gestión académica y administrativa para el fortalecimiento institucional. Se determinó que la implementación de asistentes algorítmicos optimizó tiempos y recursos, al encargarse del procesamiento masivo de datos y presentar resultados depurados. La investigación destacó que estas herramientas liberaron al capital humano de tareas repetitivas, permitiendo a los administradores enfocar su criterio en la evaluación cualitativa de los reportes generados y en la gobernanza transparente de la institución.

La incorporación técnica de la IA en la estructura organizacional se canalizó a través de los Sistemas de Soporte a la Decisión (DSS). Shwedeh \cite{shwedeh_integration_2024} investigó la integración de la IA en los DSS dentro de instituciones de educación superior, utilizando la Teoría de Difusión de Innovaciones. Se comprobó que la eficacia del sistema de recomendación dependió de factores como la calidad de los datos y la preparación organizacional. El estudio reveló que, una vez superada la barrera de complejidad inicial, la IA potenció la capacidad analítica de los directivos, proporcionando evaluaciones de riesgo y proyecciones que sirvieron como base fundamental para resolver problemas no estructurados de planificación académica.

Sin embargo, se identificó que la tecnología de soporte resultó insuficiente sin una base de conocimiento estructurada. Abed Alshadoodee et al. \cite{abed_alshadoodee_role_2022} exploraron el rol de la IA en la mejora de los DSS administrativos dependiendo de la Gestión del Conocimiento (KM). Se demostró que la función principal de la IA fue transformar datos brutos en conocimiento institucional accionable para el usuario final. Los autores concluyeron que los algoritmos de inteligencia de negocios requirieron alimentarse de un repositorio de conocimiento organizacional bien curado para ofrecer recomendaciones válidas, vinculando así la utilidad de la sugerencia algorítmica con la madurez de la gestión del conocimiento de la institución.

Al contrastar las perspectivas de Shwedeh \cite{shwedeh_integration_2024} y Abed Alshadoodee et al. \cite{abed_alshadoodee_role_2022}, se evidenció una interdependencia crítica para el diseño de la solución de asistencia. Mientras Shwedeh \cite{shwedeh_integration_2024} enfatizó la importancia de la infraestructura tecnológica y la preparación organizacional como prerrequisitos para la adopción de DSS inteligentes, Abed Alshadoodee et al. \cite{abed_alshadoodee_role_2022} establecieron que dicha infraestructura careció de valor estratégico si no se sustentó en procesos de Gestión del Conocimiento. Ambos estudios coincidieron en que el éxito de la IA administrativa no residió en la autonomía del algoritmo, sino en la integración sistémica donde la máquina procesó la complejidad estadística y el humano aplicó el conocimiento contextual para validar la decisión final.

Previo al despliegue técnico del sistema de soporte, se abordó la dimensión normativa y ética del uso de datos. Kaşarcı et al. \cite{kasarci_managing_2025} realizaron una revisión sistemática sobre la ética de la IA en la educación superior, detectando una brecha significativa entre la adopción tecnológica y la capacidad institucional para gestionarla. Se identificó que la mayoría de las respuestas institucionales fueron reactivas; por tanto, el estudio subrayó la urgencia de establecer políticas proactivas de gobernanza. Se determinó que el sistema debía operar bajo reglas claras donde la IA funcionara como un auditor de datos y generador de alertas, pero nunca como un juez final, mitigando así los riesgos legales asociados a la automatización total.

La viabilidad de la implementación también dependió de la aceptación cultural por parte de la comunidad educativa hacia la asistencia algorítmica. Rivera y Osena \cite{rivera_exploring_2025} exploraron las percepciones éticas sobre el uso de la IA y la integridad académica. Se observó que, aunque existió un reconocimiento de los beneficios de eficiencia, persistió una preocupación sobre el sesgo en las recomendaciones algorítmicas. La investigación determinó que la confianza en el sistema se correlacionó con la transparencia; por tanto, se estableció que la solución debió incluir mecanismos de explicabilidad (\textit{Explainable AI}) que permitieran al jefe de carrera entender el ``porqué'' de cada sugerencia antes de aprobarla.

En síntesis, la fundamentación teórica de la solución propuesta se cimentó en la convergencia de la eficiencia operativa y la responsabilidad humana supervisada. Se determinó que la aplicación de la IA a la gestión administrativa constituyó una optimización del proceso de toma de decisiones, soportada por Sistemas de Soporte a la Decisión (DSS) que transformaron datos en conocimiento. Una vez establecido el marco de gobernanza y definido el rol de la IA como un asistente consultivo, se procedió a abordar el componente técnico subyacente: la infraestructura de datos masivos necesaria para alimentar este motor de inteligencia.

\section{Minería de Datos Educativa (MDE) y Big Data}
La implementación de un sistema de asesoría basado en inteligencia artificial requirió el establecimiento previo de una arquitectura de datos masivos (\textit{Big Data}) robusta y escalable. Se determinó que la capacidad del sistema para ofrecer recomendaciones estratégicas al jefe de carrera no dependía únicamente de la sofisticación algorítmica, sino de la integridad y disponibilidad de los volúmenes de información histórica. Por tanto, se definió un marco de trabajo que integró la recolección, almacenamiento y procesamiento de datos heterogéneos, trascendiendo el simple reporte estadístico para constituir un ecosistema de Minería de Datos Educativos (MDE) capaz de alimentar modelos predictivos en tiempo real.

Para fundamentar esta arquitectura, se analizó el estado del arte global en la integración de tecnologías analíticas. Thayyib et al. \cite{thayyib_state_art_2023} realizaron un estudio bibliométrico exhaustivo sobre la convergencia entre Inteligencia Artificial y Analítica de Big Data (\textit{Big Data Analytics - BDA}) en diversos dominios. Se identificó que la tendencia actual se alejó de las soluciones aisladas para adoptar enfoques unificados donde el BDA gestionó el volumen, la velocidad y la variedad de los datos, mientras que la IA aportó la capa cognitiva. El estudio concluyó que las arquitecturas exitosas fueron aquellas que lograron orquestar estos dos componentes para transformar datos crudos en ``inteligencia accionable'', validando así el diseño de una plataforma híbrida para la gestión universitaria.

Específicamente en el ámbito de la gestión, se adoptó el marco de referencia de la Minería de Datos Educativos (MDE) orientada a la administración. Almaghrabi et al. \cite{almaghrabi_sok_2024} sistematizaron el conocimiento (SoK) sobre el impacto de la MDE en la administración organizacional, definiéndola como el proceso de descubrir patrones ocultos en grandes conjuntos de datos educativos para optimizar la toma de decisiones. Se estableció que, a diferencia de la MDE pedagógica (centrada en el aprendizaje), la MDE administrativa se enfocó en variables de eficiencia operativa y asignación de recursos. La investigación determinó que la aplicación de estas técnicas permitió a las organizaciones transitar de una administración basada en la intuición a una gestión basada en evidencia empírica robusta.

La materialización de estos conceptos se proyectó sobre los Sistemas de Gestión de Información (IMS) universitarios existentes. Zhang et al. \cite{zhang_data_2022} exploraron las aplicaciones de minería de datos en el desarrollo de IMS, proponiendo el uso de algoritmos de asociación y clasificación integrados directamente en la base de datos institucional. Se implementó una lógica de extracción donde los datos de matrícula, rendimiento y asistencia no se analizaron de forma aislada, sino como componentes interconectados de un sistema complejo. El estudio demostró que integrar la minería de datos en el núcleo del IMS permitió detectar correlaciones no evidentes como la relación entre la carga horaria y la deserción, proporcionando insumos críticos para la planificación académica.

Para el procesamiento técnico de los datos, se adaptó un flujo de trabajo (pipeline) de analítica avanzado. Aunque originalmente diseñado para la automatización de edificios, Himeur et al. \cite{himeur_ai-big_2023} definieron una arquitectura de ``AI-big data analytics pipeline'' cuya estructura modular de adquisición, preprocesamiento, capa de análisis y capa de decisión resultó plenamente transferible al contexto educativo. Se adoptó este esquema para garantizar que el flujo de datos desde la captura de registros históricos hasta la generación de la sugerencia siguiera un proceso estandarizado de limpieza y normalización. Se determinó que este enfoque ingenieril fue indispensable para manejar la heterogeneidad de las fuentes de datos universitarias, asegurando que el modelo de IA recibiera información depurada y consistente.

La arquitectura técnica se alineó con las necesidades estratégicas de los tomadores de decisiones. Rabelo et al. \cite{rabelo_educational_2024} aportaron la ``perspectiva gerencial'' de la minería de datos y la analítica de aprendizaje, enfatizando que la recolección de datos debía estar subordinada a objetivos de efectividad institucional. Se estableció que el sistema no debía abrumar al jefe de carrera con métricas crudas, sino presentar indicadores sintetizados que facilitaran la evaluación de la eficiencia del plan de estudios. El estudio concluyó que el éxito de la implementación dependió de la capacidad del sistema para traducir la complejidad del \textit{Big Data} en tableros de control intuitivos que respaldaran la gestión estratégica.

Finalmente, la validación del marco de trabajo de datos masivos se fundamentó en la intersección entre la capacidad operativa y la utilidad estratégica. Al integrar los postulados de Almaghrabi et al. \cite{almaghrabi_sok_2024} con la visión de Rabelo et al. \cite{rabelo_educational_2024}, se consolidó un modelo donde la robustez técnica sirvió a la claridad administrativa. Mientras Almaghrabi et al. \cite{almaghrabi_sok_2024} detallaron cómo la MDE podía descubrir ineficiencias ocultas en la administración organizacional, Rabelo et al. \cite{rabelo_educational_2024} definieron cómo presentar esos hallazgos para que fueran ``gerencialmente consumibles''. Ambos estudios coincidieron en que el \textit{Big Data} educativo debía funcionar como un puente entre la realidad operativa de los registros académicos y la visión estratégica de la dirección, justificando así la inversión en infraestructura de datos como un activo de inteligencia institucional.

\section{Ciclo de vida de los datos: Adquisición y Preparación}
El ciclo de vida de los datos se concibió como un proceso de ingeniería sistemático, diseñado para transformar la heterogeneidad de los registros académicos en activos de información estructurada aptos para el modelado algorítmico. Se estableció que la calidad de las predicciones de la inteligencia artificial dependía intrínsecamente de la higiene de los datos de entrada; por tanto, se implementó una arquitectura de tuberías de datos (pipelines) que estandarizó las fases de extracción, transformación y carga (ETL). Este enfoque permitió transitar de una manipulación manual y fragmentada a un flujo automatizado, garantizando que el sistema de soporte a la decisión se alimentara de información veraz, consistente y actualizada en tiempo real.

Para la orquestación de este flujo, se adoptó el paradigma de pipelines ETL escalables orientados a la inteligencia de negocios. Mahmud e Ikbal \cite{master_of_science_in_information_technology_msit_washington_university_of_science_and_technology_alexandria_va_22314_usa_role_2022} realizaron un estudio comparativo sobre herramientas de integración de datos, determinando que los pipelines modernos actúan como la columna vertebral de cualquier sistema de inteligencia escalable. Se definió una arquitectura donde la extracción no se limitó a copiar datos, sino que integró conectores para múltiples fuentes (ERP, LMS); posteriormente, la fase de transformación aplicó reglas de negocio para normalizar formatos dispares. El estudio concluyó que el uso de herramientas ETL dedicadas fue superior a los scripts ad-hoc, pues garantizó la trazabilidad del dato y facilitó la gobernanza en entornos de alto volumen transaccional.

La fase de carga (Load) se dirigió hacia un esquema de base de datos relacional optimizado para el análisis educativo. Retomando el modelo de Kustitskaya et al. \cite{kustitskaya_designing_2023}, se diseñó una estructura de datos que reflejó la complejidad de los procesos universitarios. Se implementaron tablas dimensionales para estudiantes, asignaturas y docentes, vinculadas a tablas de hechos que registraron eventos académicos (calificaciones, asistencias). Esta estructuración permitió que los datos, una vez procesados por el ETL, residieran en un formato que facilitaba las consultas analíticas complejas, asegurando que el motor de IA pudiera acceder rápidamente a las relaciones históricas entre el rendimiento estudiantil y la carga académica sin latencia excesiva.

La operatividad continua del sistema se aseguró mediante la automatización integral de estos procesos. Ravi \cite{ravi_etl_2025} analizó la evolución de la integración de datos, destacando que los sistemas ETL tradicionales enfrentaban limitaciones de velocidad y escalabilidad que solo la automatización podía resolver. Se implementaron mecanismos de triggers (disparadores) y programación por lotes (\textit{batch processing}) que ejecutaron las tareas de limpieza y carga sin intervención humana. La investigación determinó que esta automatización fue crítica para la toma de decisiones en tiempo real, ya que redujo drásticamente el tiempo de latencia entre la generación del dato académico y su disponibilidad para los algoritmos predictivos.

Una vez centralizados los datos, se procedió a una etapa de preparación específica para los modelos de decisión. Bas et al. \cite{bas_multi-criteria_2025} propusieron metodologías de preprocesamiento para sistemas de soporte a la decisión multicriterio, enfatizando la necesidad de normalizar las variables para hacerlas comparables. Se aplicaron técnicas de estandarización a los indicadores numéricos (como promedios de notas o tasas de ocupación) para eliminar sesgos derivados de las diferentes escalas de medición. El estudio validó que esta preparación rigurosa fue indispensable para definir ``grupos de control'' y perfiles comparables, asegurando que el sistema de IA no ponderara erróneamente una variable simplemente por tener una magnitud numérica mayor.

Para optimizar el rendimiento de los modelos predictivos, se implementó una estrategia de reducción de dimensionalidad y limpieza de ruido. Awad y Fraihat \cite{awad_recursive_2023} demostraron la eficacia de la Eliminación Recursiva de Características (\textit{Recursive Feature Elimination - RFE}) combinada con validación cruzada para seleccionar los atributos más relevantes. Se aplicó este algoritmo para filtrar variables redundantes o irrelevantes del conjunto de datos educativo, reteniendo únicamente aquellas que aportaban valor predictivo real (como el historial de reprobación o la carga horaria previa). La investigación concluyó que el uso de RFE no solo redujo el costo computacional del entrenamiento, sino que mejoró la precisión de los clasificadores al eliminar el ``ruido'' estadístico.

La integración de una arquitectura ETL robusta con técnicas avanzadas de selección de características consolidó el marco de ingeniería de datos del proyecto. Al contrastar los hallazgos de Mahmud e Ikbal \cite{master_of_science_in_information_technology_msit_washington_university_of_science_and_technology_alexandria_va_22314_usa_role_2022} con la metodología de Ravi \cite{ravi_etl_2025}, se evidenció que la sostenibilidad del sistema dependió de su capacidad para escalar automáticamente ante el crecimiento del volumen de datos históricos. Mientras Mahmud e Ikbal \cite{master_of_science_in_information_technology_msit_washington_university_of_science_and_technology_alexandria_va_22314_usa_role_2022} proveyeron el diseño estructural para la integración de fuentes heterogéneas, Ravi \cite{ravi_etl_2025} aportó la lógica de automatización necesaria para el mantenimiento del flujo. Esta convergencia garantizó que el motor de inferencia recibiera un suministro constante de datos de alta calidad, procesados eficientemente para respaldar la toma de decisiones estratégicas.

\section{Aprendizaje Automático}
El Aprendizaje Automático (\textit{Machine Learning}) se estableció como el motor computacional central del sistema propuesto, permitiendo la transición de una programación académica estática a una dinámica. A diferencia de los algoritmos deterministas tradicionales, que siguen reglas fijas, se implementaron modelos capaces de aprender patrones complejos a partir de datos históricos de matrícula y rendimiento. Se definió que la función de estos algoritmos no sería reemplazar el criterio humano, sino potenciarlo mediante la detección de tendencias no lineales en la demanda estudiantil, facilitando una personalización de la oferta que hubiera sido inabarcable mediante el cálculo manual.

La aplicación de estas técnicas se alineó con las tendencias más recientes en la educación superior. Peng y Li \cite{peng_frontiers_2025} realizaron una revisión sistemática de artículos líderes sobre inteligencia artificial, identificando un cambio de paradigma hacia el aprendizaje personalizado. Se observó que los algoritmos de aprendizaje automático permitieron adaptar los recursos educativos a las necesidades individuales de los estudiantes, prediciendo rutas de éxito y riesgos de deserción. El estudio concluyó que la integración de modelos predictivos en la gestión institucional fue el factor diferenciador que permitió a las universidades modernas ofrecer trayectorias curriculares flexibles y centradas en el estudiante, validando así la pertinencia técnica del enfoque propuesto.

\subsection{Conceptos clave: Entrenamiento, Pruebas y Validación}
La fiabilidad de los modelos predictivos se fundamentó en una metodología rigurosa de entrenamiento y validación. Se determinó que el simple ajuste de un algoritmo a los datos disponibles no garantizaba su capacidad de generalización ante futuros escenarios de matrícula; por tanto, se establecieron protocolos estrictos de separación de datos. El proceso de entrenamiento implicó la partición del conjunto de datos en subconjuntos de aprendizaje y prueba, asegurando que el modelo fuera evaluado con información que no había ``visto'' previamente, mitigando así el riesgo de sobreajuste (\textit{overfitting}) y garantizando predicciones robustas en el entorno real.

Para asegurar la estabilidad de las métricas de rendimiento, se implementó la estrategia de validación cruzada. Sweet et al. \cite{sweet_cross-validation_2023} demostraron que la elección de la estrategia de validación impacta significativamente en la interpretación del desempeño del modelo, especialmente en datos con correlaciones espaciales o temporales. Se adoptó un enfoque de validación cruzada (como \textit{k-fold}) que permitió evaluar el modelo en múltiples particiones de los datos históricos. La investigación estableció que esta técnica fue indispensable para obtener una estimación insesgada del error de predicción, evitando que resultados aparentemente exitosos fueran producto de una partición de datos afortunada o aleatoria.

La maximización de la precisión del modelo requirió un ajuste fino de sus configuraciones internas, conocido como optimización de hiperparámetros. Al integrar la metodología de Sweet et al. \cite{sweet_cross-validation_2023} con el enfoque de El-Shahat et al. \cite{el-shahat_machine_2024}, se configuró un flujo de trabajo robusto. Mientras Sweet et al. \cite{sweet_cross-validation_2023} proveyeron el marco de validación para evaluar cada configuración, El-Shahat et al. \cite{el-shahat_machine_2024} validaron el uso de la Búsqueda en Cuadrícula (\textit{Grid Search}) combinada con validación cruzada (\textit{Grid Search Cross-Validation - GSCV}) para encontrar la combinación óptima de parámetros. Ambos estudios coincidieron en que la aplicación sistemática de GSCV permitió explorar exhaustivamente el espacio de búsqueda del algoritmo, garantizando que el modelo final operara en su punto de máximo rendimiento matemático.

\subsection{Algoritmos de Regresión}
Para la predicción de la demanda de cupos, se seleccionaron algoritmos de regresión debido a la naturaleza cuantitativa de la variable objetivo. Se analizó que la estimación de la matrícula no podía tratarse como un problema de clasificación binaria, sino como la predicción de un valor continuo que fluctuaba según múltiples variables independientes. En consecuencia, se priorizaron modelos capaces de capturar relaciones no lineales entre los atributos académicos y demográficos, descartando regresiones lineales simples en favor de métodos de ensamble y modelos basados en aprendizaje profundo que ofrecieron una mayor precisión en entornos de alta varianza.

Uno de los algoritmos principales implementados fue el \textit{Random Forest Regressor}, reconocido por su robustez frente al ruido en los datos. El Mrabet et al. \cite{el_mrabet_random_2022} detallaron el funcionamiento de este algoritmo basado en el ensamble de múltiples árboles de decisión, donde la predicción final se obtiene promediando los resultados de árboles individuales. Se seleccionó este modelo por su capacidad intrínseca para manejar grandes conjuntos de datos sin incurrir en sobreajuste, gracias a la aleatoriedad en la selección de características. El estudio validó que \textit{Random Forest} ofreció un equilibrio superior entre precisión y estabilidad, siendo ideal para predecir variables críticas donde el costo del error es alto.

Complementariamente, se exploró el uso de arquitecturas avanzadas para la extracción de métricas predictivas a partir de datos estructurados y semiestructurados. Hamill e Iqbal \cite{hamill_regression-based_2026} propusieron un enfoque innovador utilizando Modelos de Lenguaje Pequeños (\textit{Small Language Models - SLM}) equipados con cabezales de regresión (\textit{regression heads}). Esta arquitectura permitió procesar no solo los valores numéricos tradicionales, sino también interpretar el contexto de los datos antes de realizar la predicción numérica. La investigación de 2026 demostró que integrar capacidades de comprensión semántica en tareas de regresión mejoró la precisión en escenarios donde los datos de entrada presentaban ambigüedad o complejidad estructural.

\subsection{Algoritmos de Clasificación}
Complementando los modelos de regresión, se implementaron algoritmos de clasificación para abordar la categorización cualitativa de los fenómenos académicos. Se determinó que, para la toma de decisiones estratégicas, a menudo resultó más valioso clasificar una asignatura o un estudiante dentro de un nivel de riesgo (Alto, Medio, Bajo) que predecir un valor numérico exacto. En este contexto, la tarea de clasificación se configuró para identificar patrones que separaran clases discretas dentro del espacio de datos, permitiendo al sistema generar alertas tempranas sobre deserción estudiantil o cuellos de botella curriculares con base en probabilidades de pertenencia a una categoría específica.

Como base fundamental para la clasificación, se analizaron los árboles de decisión por su capacidad de interpretación. Blockeel et al. \cite{blockeel_decision_2023} examinaron la evolución de estos algoritmos desde la predicción eficiente hasta la ``Inteligencia Artificial Responsable''. Se estableció que, aunque los modelos de caja negra (como las redes neuronales profundas) suelen ofrecer alta precisión, los árboles de decisión permitieron trazar la lógica exacta detrás de cada clasificación. Esta característica de transparencia se consideró crítica para el componente de asesoría del sistema, ya que facilitó la justificación de las recomendaciones ante los gestores académicos, cumpliendo con el requisito de explicabilidad ética en la gestión de datos sensibles.

Para superar las limitaciones de estabilidad de un árbol individual, se adoptó el algoritmo \textit{Random Forest}. Salman et al. \cite{salman_random_2024} presentaron una visión general técnica de este método de ensamble, el cual construye múltiples árboles de decisión durante el entrenamiento y fusiona sus resultados (\textit{bagging}). Se analizó que esta técnica redujo significativamente la varianza y el riesgo de sobreajuste (\textit{overfitting}) inherente a los árboles simples. El estudio destacó la capacidad del algoritmo para manejar grandes volúmenes de datos con alta dimensionalidad y valores perdidos, características típicas de los registros administrativos universitarios incompletos o heterogéneos.

La superioridad de los métodos de ensamble en el contexto educativo se validó al cruzar los hallazgos de Gusnina et al. \cite{gusnina_student_2022} con los de Bujang et al. \cite{bujang_multiclass_2021}. Mientras Gusnina et al. \cite{gusnina_student_2022} demostraron en un caso de estudio en la Universidad Sebelas Maret que el \textit{Random Forest} alcanzó una precisión del 90.7\% en la predicción del rendimiento, superando a otros clasificadores clásicos; Bujang et al. \cite{bujang_multiclass_2021} expandieron esta aplicación hacia modelos de predicción multiclase para estimar calificaciones específicas. Ambos estudios coincidieron en que la selección de características basada en la ganancia de información (\textit{Information Gain}) fue determinante para el éxito del modelo. Se concluyó que \textit{Random Forest} ofreció el balance óptimo entre precisión predictiva y capacidad para manejar múltiples categorías de resultados académicos.

Finalmente, se evaluó la pertinencia de las Máquinas de Soporte Vectorial (SVM) para escenarios de clasificación con márgenes de decisión complejos. Valkenborg et al. \cite{valkenborg_support_2023} detallaron los fundamentos matemáticos de las SVM, explicando su funcionamiento mediante la construcción de hiperplanos que maximizan la separación entre clases. Aunque se reconoció su alta efectividad en espacios de gran dimensión, se determinó que su costo computacional cuadrático limitaba su escalabilidad frente a grandes bases de datos históricas en comparación con los métodos de árboles. No obstante, se mantuvo como un algoritmo de referencia para la validación cruzada de los resultados obtenidos por los modelos principales.

\section{Métricas de evaluación y validación de modelos}
La validación del rendimiento de los modelos no se limitó a un ejercicio estadístico convencional, sino que se configuró como un mecanismo de aseguramiento de la calidad para el sistema de soporte a la decisión. Se determinó que, para que el algoritmo funcionara como un asesor confiable para el jefe de carrera, era imperativo medir no solo su tasa de aciertos global, sino su comportamiento frente a clases desbalanceadas y la magnitud de sus errores en la estimación de cupos. Por consiguiente, se seleccionó una batería de métricas diferenciadas para las tareas de clasificación (riesgo de deserción) y regresión (proyección de matrícula), priorizando aquellas que penalizaron los ``falsos negativos'', dado que omitir un riesgo crítico se consideró más costoso institucionalmente que una falsa alarma.

En el ámbito de la clasificación de riesgos académicos, se enfrentó el desafío de los conjuntos de datos desbalanceados, donde la ``clase minoritaria'' (estudiantes en riesgo) solía ser la de mayor interés estratégico. Sujon et al. \cite{sujon_accuracy_2025} presentaron evidencia empírica reciente sobre la evaluación de modelos predictivos en dominios de alto impacto, cuestionando el uso de la ``Exactitud'' (\textit{Accuracy}) como métrica única. Su investigación demostró que métricas como el \textit{F1-Score} y el Coeficiente de Correlación de Matthews (MCC) ofrecieron una fiabilidad superior al evaluar el desempeño del modelo frente a clases minoritarias. Se adoptó el MCC como el estándar de validación, ya que este coeficiente cuantificó la calidad de la predicción binaria considerando verdaderos y falsos positivos y negativos, asegurando que el sistema no sesgara sus recomendaciones hacia la mayoría de estudiantes aprobados.

Para los modelos de regresión encargados de predecir la cantidad exacta de cupos, se analizó la naturaleza del error de estimación. Al contrastar la teoría de Hodson \cite{hodson_root-mean-square_2022} con la aplicación práctica de Shao et al. \cite{shao_machine_2021}, se definió el criterio de evaluación final. Hodson \cite{hodson_root-mean-square_2022} argumentó que la elección entre la Raíz del Error Cuadrático Medio (RMSE) y el Error Absoluto Medio (MAE) a menudo se presentó como una falsa dicotomía, aclarando que el RMSE es más sensible a errores grandes (valores atípicos), mientras que el MAE ofrece una interpretación más lineal del error promedio. Por su parte, Shao et al. \cite{shao_machine_2021} aplicaron estas métricas específicamente para minimizar costos administrativos por subestimación de matrícula. Se concluyó utilizar ambas métricas en conjunto: el MAE para reportar el error esperado ``en el día a día'' al usuario, y el RMSE para calibrar el modelo durante el entrenamiento, penalizando severamente las desviaciones grandes que podrían causar una crisis de saturación en las aulas.

Finalmente, la selección de estas métricas se alineó con el objetivo último de mejorar el logro estudiantil. Wang et al. \cite{wang_analysis_2022} utilizaron el aprendizaje automático para analizar factores de influencia en el rendimiento, demostrando que la precisión del modelo predictivo tuvo una correlación directa con la efectividad de las intervenciones académicas. Se estableció que un sistema validado con métricas robustas permitió identificar con mayor certeza los factores determinantes del éxito o fracaso. Esto garantizó que las sugerencias emitidas por el sistema de soporte a la decisión no fueran meras especulaciones estadísticas, sino inferencias validadas capaces de orientar políticas de oferta académica más justas y eficientes.

\section{Interfaz de Programación de Aplicaciones y consumo de datos}
La operacionalización de los modelos predictivos y de clasificación no se concibió como una ejecución de scripts aislados, sino que se estructuró mediante una arquitectura de software orientada a servicios. Se determinó que, para que el sistema de soporte a la decisión fuera accesible y escalable, la lógica de inteligencia artificial debía encapsularse detrás de una Interfaz de Programación de Aplicaciones (API) estandarizada. Esta capa de intermediación permitió desacoplar el núcleo de procesamiento matemático de las interfaces de usuario, garantizando que el consumo de los datos procesados ya fueran predicciones de matrícula o alertas de riesgo se realizara de manera segura, eficiente y agnóstica a la plataforma cliente utilizada por las autoridades universitarias.

Para soportar la complejidad del sistema, se evaluó la transición hacia una arquitectura distribuida. Blinowski et al. \cite{blinowski_monolithic_2022} realizaron una evaluación comparativa de rendimiento entre arquitecturas monolíticas y de microservicios, determinando que, si bien los monolitos ofrecen simplicidad inicial, los microservicios garantizan una escalabilidad horizontal superior y una mayor tolerancia a fallos. Basándose en este análisis, se implementó una arquitectura donde los módulos de predicción (regresión) y clasificación (riesgo) operaron como servicios independientes. Esta decisión permitió que el entrenamiento intensivo de un modelo no degradara el rendimiento del resto del sistema, asegurando una alta disponibilidad durante los periodos críticos de matrícula.

El diseño de los puntos de acceso (\textit{endpoints}) para el consumo de datos se rigió por estándares de industria para asegurar la interoperabilidad. Gowda y Gowda \cite{gowda_best_2024} establecieron las mejores prácticas en el diseño de APIs RESTful, enfatizando la importancia de convenciones de nomenclatura semántica y protocolos de seguridad robustos. Se adoptó este enfoque para exponer los resultados de la IA, implementando mecanismos de autenticación y encriptación en el transporte de datos. El estudio validó que una API bien diseñada no solo facilita la integración con sistemas legados (como el ERP universitario), sino que actúa como una barrera de seguridad, protegiendo los datos sensibles de los estudiantes contra accesos no autorizados durante la consulta.

La infraestructura física para el alojamiento de estos servicios se definió bajo un modelo de soberanía de datos. Maaz et al. \cite{mohammed_maaz_development_2023} analizaron el desarrollo de modelos de despliegue de servicios en nubes privadas, destacando que este entorno ofrece un control granular sobre la privacidad y el cumplimiento normativo que las nubes públicas no siempre garantizan. Se decidió desplegar la solución en una infraestructura de nube privada institucional, lo que permitió cumplir con las regulaciones de protección de datos académicos. Esta estrategia aseguró que, aunque el sistema utilizara técnicas avanzadas de computación en la nube para escalar, la información confidencial nunca abandonara el perímetro de control de la universidad.

La gestión dinámica de los recursos computacionales requeridos por los modelos de IA se resolvió mediante tecnologías de orquestación de contenedores. Al integrar la visión de Senjab et al. \cite{senjab_survey_2023} con la evaluación de Blinowski et al. \cite{blinowski_monolithic_2022}, se consolidó una plataforma de ejecución resiliente. Senjab et al. \cite{senjab_survey_2023} detallaron cómo los algoritmos de planificación de Kubernetes optimizan la asignación de \textit{pods} (contenedores) en función de la carga de trabajo en tiempo real, mientras que Blinowski et al. \cite{blinowski_monolithic_2022} confirmaron que esta capacidad de autoescalado es la ventaja competitiva clave de los microservicios frente a cargas variables. Ambos estudios coincidieron en que el uso de orquestadores permitió al sistema responder elásticamente: asignando más potencia de cálculo durante las semanas de inscripción y reduciéndola en periodos lectivos, optimizando así el costo operativo de la infraestructura.

Finalmente, la integración de estas APIs marcó el hito definitivo en la modernización tecnológica propuesta. Nazyrova et al. \cite{nazyrova_digital_2025} exploraron la transformación digital de la educación superior en el contexto de un futuro impulsado por la IA, concluyendo que la interoperabilidad es el factor crítico de éxito. Se estableció que el valor real del sistema desarrollado no residía únicamente en sus algoritmos internos, sino en su capacidad para dialogar fluidamente con el ecosistema digital universitario existente. La API actuó como el catalizador que permitió transformar la ``Universidad Digital'' (que solo digitaliza papeles) en una ``Universidad Inteligente'' (que conecta datos para decidir), cerrando la brecha entre la gestión administrativa y la innovación pedagógica.

En conclusión, el diseño de la solución tecnológica abordado en este capítulo estableció una base ingenieril sólida, transitando desde la fundamentación ética y estratégica (DSS), pasando por la arquitectura de datos masivos (\textit{Big Data}/ETL) y el núcleo algorítmico (ML), hasta culminar en una interfaz de consumo segura y escalable (API). Se demostró teóricamente que la integración de estos componentes conforma un sistema capaz de mitigar las problemáticas de planificación detectadas. Con la arquitectura definida y los modelos seleccionados, se procedió a la fase de experimentación y validación empírica de los resultados, cuyo análisis detallado se presenta en el capítulo siguiente.