\chapter[Metodología]{Metodología}
\label{ch:metodologia}

\insertminitoc
\parindent0pt

\section{Tipo de investigación}
La investigación se clasificó bajo un enfoque cuantitativo con un alcance correlacional y predictivo. Hernández-Sampieri, Fernández Collado y Baptista Lucio\cite{sampieri_metodologia_2014} establecieron que los estudios correlacionales tienen como propósito principal conocer la relación o grado de asociación que exista entre dos o más conceptos, categorías o variables en un contexto en particular. Se determinó que el proyecto buscó establecer la relación matemática entre el comportamiento histórico de matrícula, las variables demográficas, los resultados del censo estudiantil y la demanda real de cupos para la carrera de Ingeniería en Sistemas. Al cuantificar estas interacciones mediante modelos de aprendizaje automático, el estudio trascendió la mera descripción del fenómeno de saturación, permitiendo pronosticar escenarios operativos futuros con un margen de error estadísticamente medible.

Asimismo, por su naturaleza final, el estudio se definió como una investigación aplicada orientada a la innovación tecnológica. Aunque la literatura metodológica tradicional \cite{sampieri_metodologia_2014} se enfoca en el desarrollo de conocimiento puro, la estructuración de esta investigación se dirigió a la resolución de un problema logístico práctico dentro de la institución. Se desarrolló un modelo algorítmico que culminó en un Sistema de Soporte a la Decisión (DSS) plenamente funcional, diseñado para dotar a las autoridades académicas de una herramienta basada en evidencia empírica para la mitigación eficiente de cuellos de botella en la oferta curricular.

\section{Diseño Metodológico}
El diseño metodológico seleccionado para la evaluación y entrenamiento de los algoritmos fue de carácter no experimental, de corte longitudinal y evolución de grupo (cohortes). Hernández-Sampieri et al. \cite{sampieri_metodologia_2014} definieron los diseños no experimentales como aquellos estudios que se realizan sin la manipulación deliberada de variables, en los cuales se observan los fenómenos tal como se dan en su contexto natural para posteriormente analizarlos. Se estableció que el modelo de inteligencia artificial fue alimentado, entrenado y validado utilizando registros históricos masivos extraídos de las bases de datos del sistema de gestión universitaria. En ningún momento se alteraron retroactivamente las calificaciones, los historiales o las pautas de inscripción de los estudiantes para forzar resultados en el algoritmo, garantizando un entorno de prueba ex post facto riguroso. 

Adicionalmente, se aplicó la dimensión longitudinal debido a la necesidad de evaluar el fenómeno a lo largo del tiempo. Según Hernández-Sampieri et al. \cite{sampieri_metodologia_2014}, los diseños longitudinales recolectan datos en diferentes puntos del tiempo para realizar inferencias respecto al cambio, sus determinantes y consecuencias. La cadencia del plan de estudios y las tasas de aprobación en asignaturas críticas se analizaron a través de múltiples periodos académicos consecutivos. Este diseño permitió al motor de pronóstico (como el suavizado exponencial y el modelo Random Forest) capturar con precisión matemática la estacionalidad de la demanda —como el incremento de estudiantes de primer ingreso en el primer ciclo del año— y el efecto de retardo causado por las clases de alta reprobación.

\section{Involucrados en la investigación}
La viabilidad e implementación de la solución arquitectónica requirió la identificación y categorización de los actores clave dentro del ecosistema universitario, evaluando su nivel de interacción, aporte de datos y el impacto que la tecnología generaría sobre sus funciones cotidianas. Reconocerlos permitió comprender de manera más precisa cómo se estructura y se comporta la planificación académica, y facilitó la interpretación de los datos recolectados.

La caracterización de estos actores también contribuyó a contextualizar los resultados obtenidos mediante los modelos predictivos y las simulaciones espaciotemporales realizadas, ya que cada grupo aportó información, necesidades y restricciones distintas sobre el funcionamiento real de la carrera. De esta forma, identificar a los involucrados se convirtió en un paso clave para analizar el ecosistema de toma de decisiones de manera integrada y coherente.

\subsection{Principales involucrados}
El primer grupo principal correspondió a la Jefatura de la Carrera y al equipo de coordinación académica. Se determinó que estos actores fungieron como los usuarios finales y directos del tablero de control analítico. Su involucramiento fue indispensable para la validación del sistema, ya que el algoritmo se diseñó para no ejecutar publicaciones autónomas en el portal de registro. En su lugar, el modelo emitió proyecciones y sugerencias de programación que la jefatura debió auditar, ajustar y aprobar, ejerciendo su criterio experto y aplicando las normativas institucionales sobre las predicciones de la máquina.

El segundo grupo de máxima prioridad estuvo conformado por la población estudiantil matriculada en la carrera. Se estableció que los estudiantes interactuaron activamente con el ecosistema de recolección de datos a través del llenado del Censo Académico Inteligente. Paralelamente, constituyeron los beneficiarios principales del diseño, puesto que la reingeniería de la programación académica, la minimización de solapamientos horarios y la garantía algorítmica de cupos en asignaturas "filtro" permitieron reducir su tiempo de permanencia en la institución y mejorar su fluidez en el progreso de la malla curricular.

\subsection{Otros involucrados}
El personal docente adscrito al departamento figuró como un grupo involucrado secundario de alto impacto logístico. Se analizó que la asignación de la carga laboral de los catedráticos estuvo directamente vinculada a los resultados arrojados por el optimizador espacial y temporal. Las normativas de contratación, la disponibilidad de jornadas y las preferencias horarias del profesorado operaron como un vector de restricciones (duras y blandas) que el algoritmo metaheurístico debió resolver para generar la matriz final del horario sin colisiones.

Finalmente, la Dirección de Ingreso, Permanencia y Promoción (o entidad homóloga encargada del registro universitario) actuó como el proveedor infraestructural clave. Su involucramiento se centró en la administración técnica del Sistema de Gestión de Información (IMS) base. De estos repositorios institucionales se extrajeron las sábanas de datos, expedientes académicos y estadísticas de aulas físicas necesarias para ejecutar las tuberías de Extracción, Transformación y Carga (ETL) que nutrieron de información veraz a los modelos de pronóstico.

\section{Técnicas e Instrumentos de recolección de datos}
La recolección de datos constituye una fase fundamental dentro de la metodología de esta investigación, 
ya que permite obtener la información base para el análisis, modelado y simulación del tráfico vehicular 
en la ciudad de Comayagua. Los datos empleados han sido obtenidos a partir de 
{\textbf{observaciones directas}} y estimaciones basadas en la experiencia personal del investigador, complementadas 
con fuentes secundarias de carácter técnico y municipal. Este enfoque, si bien limita la precisión 
absoluta de los datos, proporciona una aproximación razonable a las condiciones reales del entorno vial, 
suficiente para efectos de simulación y análisis en entornos virtuales como \acrshort{sumo}.

La técnica principal utilizada fue la observación sistemática no participante, la cual consistió en 
registrar manualmente los comportamientos del tráfico durante diferentes franjas horarias (pico matutino, 
mediodía y vespertino), en puntos estratégicos de la ciudad donde se concentra la mayor afluencia 
vehicular. Estos puntos incluyeron las principales arterias viales de Comayagua, como el bulevar Roberto
Romero Larios, la carretera CA-5. En cada sitio se observó la densidad del tráfico, los tipos de 
vehículos predominantes, las velocidades promedio y los tiempos aproximados de espera en intersecciones.
Además de la observación, se aplicó una técnica de estimación de flujos vehiculares basada en el conteo 
visual y la proyección por intervalos de tiempo. Esta técnica consistió en medir la cantidad promedio de 
vehículos que transitan por una vía durante lapsos de 10 a 15 minutos, para luego extrapolar los valores 
a una hora completa.

En cuanto a los instrumentos utilizados, se emplearon herramientas digitales que facilitaron la 
recopilación y organización de la información. Entre ellas destacan Google Maps, OpenStreetMap y Waze, 
utilizadas para identificar y delimitar las áreas de estudio, obtener coordenadas geográficas precisas y 
trazar rutas de desplazamiento.
También se recurrió al software \acrshort{sumo} como instrumento de análisis indirecto, ya que su módulo de 
generación de tráfico (\textbf{RandomTrips.py}) permitió contrastar los valores observacionales con escenarios 
simulados. De esta forma, los datos recolectados manualmente se tradujeron en parámetros digitales 
(rutas, velocidades, tasas de inserción vehicular), integrándose dentro de un modelo virtual del tráfico 
de Comayagua. Esta fase de digitalización de datos fue fundamental para vincular la observación empírica 
con el análisis cuantitativo, garantizando así la coherencia metodológica del estudio.

Finalmente, el uso de técnicas mixtas de registro observación directa, conteo visual y modelado en 
software proporcionó una base sólida para el análisis comparativo entre el flujo vehicular observado y 
los resultados generados por la simulación. Si bien las limitaciones inherentes a la falta de 
instrumentación electrónica reducen la exactitud en términos absolutos, el objetivo central del estudio 
no radica en la medición exacta, sino en la evaluación de la aplicabilidad de la inteligencia artificial 
como herramienta de optimización del tráfico vehicular. Por tanto, las técnicas de recolección empleadas 
son adecuadas para demostrar la viabilidad práctica del modelo propuesto en contextos urbanos con 
recursos tecnológicos limitados, como el caso de Comayagua.

\section{Procedimiento}
El procedimiento de esta investigación se estructuró mediante una secuencia metodológica compuesta por diversas fases interrelacionadas, diseñadas para garantizar un proceso 
sistemático, replicable y alineado con los objetivos propuestos. Cada fase aborda un componente específico del estudio, desde la observación empírica del entorno vial hasta 
la validación final del modelo de simulación. El enfoque empleado es de carácter aplicado, puesto que busca generar información útil para la gestión del tráfico urbano en 
Comayagua mediante herramientas cuantitativas, técnicas de modelado y métodos basados en la teoría de grafos, permitiendo obtener conclusiones medibles y transferibles a 
contextos urbanos similares. \\

\textbf{Fase 1: Observación y diagnóstico del entorno vial} \\
La primera fase consistió en un reconocimiento empírico del sistema vial de Comayagua, mediante visitas de campo y observaciones directas en distintos puntos de la ciudad. Se 
identificaron las principales vías de circulación, intersecciones críticas y zonas donde se produce la mayor congestión vehicular. Durante esta etapa, se levantaron datos 
aproximados sobre la cantidad de vehículos que transitan en horas pico. Este diagnóstico inicial permitió establecer una línea base del flujo vehicular, necesaria para la 
posterior simulación del entorno urbano. \\

\textbf{Fase 2: Manejo y Ordenamiento de la Información Recopilada} \\
Tras la adquisición de los datos, estos fueron organizados y depurados meticulosamente en hojas de cálculo, categorizándolos por clase, instante de registro y coordenadas 
geográficas. A continuación, la información fue migrada a un formato que se integrara sin problemas con las plataformas de simulación en uso. Dicha organización hizo posible 
el establecimiento de variables como la congestión vehicular, la extensión de las rutas y la cadencia de tránsito. Si bien los datos se obtuvieron mediante la mera observación 
y ciertos cálculos, el tratamiento cuantitativo aseguró que los valores integrados en la simulación reflejaran de manera fiel y lógica la dinámica verdadera del tráfico. Esta 
etapa se centró en alistar los datos numéricos para nutrir el modelo informático.\\

\textbf{Fase 3: Análisis de la red vial aplicando la teoría de grafos} \\
En esta fase se procedió a representar la red vial de Comayagua mediante un grafo dirigido y ponderado, donde los nodos corresponden a intersecciones y las aristas a segmentos 
viales. A partir de esta estructura, se aplicaron métricas fundamentales de la teoría de grafos, tales como:

\begin{itemize}
    \item \textbf{Grado de entrada y salida}, para identificar intersecciones con mayor carga estructural.
    \item \textbf{Caminos mínimos}, útiles para determinar trayectos más eficientes.
    \item \textbf{Centralidades} (betweenness, closeness, degree), que permitieron reconocer nodos críticos y posibles cuellos de botella.
    \item \textbf{Componentes conexas y subgrafos relevantes}, para comprender la coherencia interna del sistema vial.
\end{itemize}

Este análisis permitió detectar vulnerabilidades, patrones topológicos y zonas de alto impacto en la dinámica de tráfico urbano. \\

\textbf{Fase 4: Modelado del entorno urbano} \\
En esta etapa se desarrolló un modelo digital del entorno vial utilizando el software \acrshort{sumo} Los mapas fueron importados desde \textbf{OpenStreetMap} y posteriormente ajustados para 
reflejar fielmente la realidad de Comayagua, incorporando sentidos de circulación, restricciones de giro, límites de velocidad, semáforos y jerarquías de vías. La estructura 
construida en \acrshort{sumo} se alineó con el grafo previamente generado, permitiendo que cada elemento del modelo matemático tuviera su equivalente en la simulación. El propósito 
principal del modelado fue disponer de una representación dinámica del sistema vial para probar escenarios y contrastar el comportamiento de la red bajo diferentes condiciones. \\


\textbf{Fase 5: Implementación de la simulación y análisis de escenarios} \\
Una vez completado el modelo urbano digital, se ejecutaron diversas simulaciones modificando condiciones como volumen vehicular, rutas predominantes y distribución del flujo. 
Estas variaciones permitieron observar indicadores como demoras, velocidad promedio y saturación de intersecciones. El análisis comparativo entre escenarios permitió examinar 
cómo la estructura del grafo influía en el rendimiento del sistema vial, y detectar puntos donde la topología de la red genera cuellos de botella o sobrecarga de flujo. En esta 
fase, la simulación se utilizó para validar las conclusiones derivadas del análisis de grafos y para estudiar el comportamiento emergente del tráfico en condiciones controladas. \\

\textbf{Fase 6: Evaluación y validación de resultados} \\
Finalmente, se llevó a cabo un proceso de validación mediante la comparación de los resultados obtenidos en la simulación con los patrones observados durante la fase de 
diagnóstico inicial. Se analizaron indicadores clave como reducción proyectada de congestión, duración de los desplazamientos y eficiencia del flujo vehicular. Esta evaluación 
buscó determinar la consistencia del modelo, su capacidad para representar adecuadamente el comportamiento real del tráfico y su utilidad para apoyar la toma de decisiones en 
la planificación vial. Los resultados confirmaron la pertinencia de utilizar teoría de grafos y simulación para identificar vulnerabilidades estructurales y proponer mejoras 
sostenibles en la movilidad urbana.

\section{Herramientas tecnológicas utilizadas}
Para llevar a cabo este estudio, fue necesario combinar diferentes instrumentos tecnológicos que 
facilitaron el manejo y estudio eficaz de la información sobre el tránsito de vehículos. Por su carácter 
cuantitativo y aplicable, las herramientas seleccionadas hicieron hincapié en la simulación del tráfico 
urbano, así como en el análisis de datos mediante \acrshort{ia}. En este apartado, se 
detallan los recursos tecnológicos más importantes que se usaron, clasificados en tres apartados: 
programas informáticos de simulación, lenguajes y espacios de programación, y plataformas de soporte 
computacional.\\

\textbf{Software de Simulación}\\
Se empleó \acrfull{sumo} para la modelación y el análisis del comportamiento del 
tráfico vehicular. A través de la configuración de rutas, cruces, semáforos y parámetros de conducción, 
este software hace posible reproducir con exactitud el flujo de vehículos en áreas urbanas. Se escoge 
\acrshort{sumo} porque es flexible, puede ser utilizado con varios formatos de datos y tiene la capacidad 
de incorporar scripts externos en Python, lo que permite comunicarse con algoritmos de \acrshort{ia}.
La creación de un mapa digital que represente la red vial de la ciudad analizada, en este caso Comayagua, 
con datos adquiridos de OpenStreetMap (OSM), es el fundamento del proceso de simulación en \acrshort{sumo}. 
Después, se crearon flujos de tráfico con cifras estimadas a partir de observaciones efectivas, 
modificando el número de vehículos, los tiempos de trayecto y los patrones de congestión. Gracias a este 
ambiente, fue posible simular situaciones de tráfico realistas, que son esenciales para ensayar y cotejar 
estrategias de optimización.\\

\textbf{Lenguajes y entornos de programación}\\
Se utilizó \texttt{Python} para analizar y procesar los datos, ya que este lenguaje tiene la capacidad de 
gestionar información y una extensa disponibilidad de bibliotecas de inteligencia artificial y 
científicas. Algunas de las librerías más utilizadas son:

\begin{itemize}
    \item \textbf{NumPy} proporcionó arreglos multidimensionales y operaciones matemáticas optimizadas, indispensables para manejar matrices, distancias, tiempos de viaje y 
    pesos asociados a las aristas del grafo vial.

    \item Por su parte, \textbf{Pandas} ofrece herramientas sofisticadas para la organización, limpieza y transformación de datos 
    tabulares, permitiendo un manejo ágil de series temporales y tablas de datos que resultan esenciales 
    para preparar la información del tráfico para su análisis posterior.

    \item \textbf{Matplotlib} se empleó como la herramienta principal de visualización del proyecto. Permitió generar gráficos estáticos, diagramas comparativos y 
    representaciones detalladas de flujos, distribuciones y métricas asociadas a la red vial. Su flexibilidad permitió ilustrar tendencias, identificar concentraciones de 
    tráfico y visualizar nodos críticos, facilitando la comprensión del comportamiento de la movilidad urbana.

    \item Dado que el eje central del estudio es la teoría de grafos, \textbf{NetworkX} fue una biblioteca fundamental. Con ella fue posible:
    
    \begin{itemize}
        \item Construir grafos dirigidos y ponderados a partir de los datos recopilados.
        \item Calcular métricas como centralidades, grado, componentes conexas y caminos mínimos.
        \item Detectar nodos vulnerables y analizar la estructura global del sistema vial.
    \end{itemize}

    NetworkX permitió traducir la información empírica en representaciones matemáticas ideales para el diagnóstico estructural del tráfico urbano.

    \item Para extraer y procesar la red vial desde OpenStreetMap se utilizó \textbf{OSMnx}, herramienta que permitió convertir datos cartográficos reales en un grafo utilizable 
    dentro del análisis computacional. A través de ella fue posible obtener automáticamente:

    \begin{itemize}
        \item Nodos (intersecciones)
        \item Aristas (segmentos viales)
        \item Sentidos de circulación
        \item Atributos como longitud, velocidad permitida y tipo de vía
    \end{itemize}

    OSMnx facilitó la integración entre datos reales y el modelado por grafos de la red vial.
\end{itemize}

\textbf{Plataformas de desarrollo y análisis}\\
Durante la fase de programación y simulación se utilizaron tres plataformas principales:

\begin{itemize}
    \item \textbf{Visual Studio Code (VS Code):} entorno de desarrollo integrado (IDE) utilizado para 
    escribir, depurar y ejecutar los scripts en Python. Su soporte para extensiones Python permitió un 
    entorno de trabajo ordenado y eficiente.
    \item \textbf{Google Colab:} plataforma en la nube utilizada para el entrenamiento de los modelos de 
    inteligencia artificial y la ejecución de procesos computacionalmente intensivos. Al ofrecer recursos 
    de GPU gratuitos, permitió mejorar los tiempos de procesamiento sin necesidad de contar con hardware 
    avanzado.
    \item \textbf{Microsoft Excel:} herramienta utilizada para organizar los datos generados durante las 
    simulaciones, elaborar tablas comparativas, calcular indicadores clave (velocidad media, flujo, 
    densidad) y crear representaciones gráficas preliminares para análisis.
\end{itemize}

\textbf{Hardware y Software}\\
El proyecto se desarrolló en una computadora portátil Lenovo Ideapad S340 15IWL, equipada con procesador 
Intel Core i5, 8 GB de memoria RAM y 500 GB de almacenamiento interno, con sistema operativo Windows 11. 
Este equipo ofreció un equilibrio adecuado entre capacidad de procesamiento y portabilidad, permitiendo 
ejecutar las simulaciones de tráfico, procesar datos y entrenar modelos de inteligencia artificial de 
manera eficiente. La configuración del hardware fue suficiente para soportar los requerimientos 
computacionales de SUMO y Python, siempre que los escenarios de simulación se mantuvieran dentro de un 
tamaño y complejidad moderados.

\section{Métodos de validación}
La \textbf{validación} constituye una de las etapas más críticas dentro del proceso metodológico, pues garantiza que el modelo propuesto y las herramientas implementadas cumplan con 
los objetivos establecidos de manera eficaz, precisa y reproducible. En el presente estudio, la validación se llevó a cabo mediante pruebas de rendimiento del modelo de 
simulación, evaluación de métricas cuantitativas y análisis de casos de uso específicos que permitieron comprobar la aplicabilidad del sistema en distintos escenarios de 
tráfico urbano en la ciudad de Comayagua.

\subsection{Métricas de evaluación del modelo}
Para asegurar que los hallazgos logrados mediante el análisis por teoría de grafos y la simulación en SUMO reflejan con precisión la conducta real del sistema vial de 
Comayagua, la validación es una fase esencial.  Para evaluar la pertinencia, la confiabilidad y la consistencia del modelo desarrollado, este estudio utilizó una mezcla de 
métodos comparativos y cuantitativos.  Estos procedimientos posibilitan comparar los patrones simulados con observaciones empíricas, comprobar la coherencia de las métricas 
topológicas y examinar el rendimiento del sistema en diferentes contextos.  Los métodos más importantes que se usaron fueron:

\subsubsection{Validación empírica de campo}
Esta técnica se basa en cotejar los resultados que el modelo ha dado (duración de trayectos, puntos de congestión) con las observaciones directas que se han recopilado 
durante la etapa de diagnóstico.

Se implementó a través de:
\begin{itemize}
    \item Comparación entre los tiempos de desplazamiento simulados y los que se han medido manualmente en horas de mayor afluencia. 
    \item Comprobación de la coincidencia entre los nodos críticos que se encuentran en el grafo y las intersecciones observadas como problemáticas. 
    \item Verificación de los patrones de congestión que se repiten y se han observado en el campo.
\end{itemize}
Este procedimiento hace posible establecer si la red modelada replica de manera apropiada la dinámica vehicular real.

\subsubsection{Validación estructural del grafo}
Se comprobó la estructura de la red vial para asegurarse de que el grafo creado cumpliera con las características esperadas de una red urbana real:
\begin{itemize}
    \item Conexión apropiada (falta de elementos desconectados en las vías principales).
    \item Correspondencia entre longitudes reales y pesos dados a los bordes.
    \item Sucesión lógica entre sentidos de circulación, limitaciones y jerarquías de las vías.
    \item Relación entre el grado de los nodos y la estructura vial que se puede observar (por ejemplo, en intersecciones primordiales o rotondas).
\end{itemize}
Antes de llevar a cabo simulaciones, este tipo de validación garantiza que el grafo represente con exactitud la estructura urbana.

\subsubsection{Validación del modelo de simulación}
Para verificar si el modelo en SUMO reproduce de manera apropiada conductas creíbles del tráfico urbano, se analizó su rendimiento.

La validación abarcó:
\begin{itemize}
    \item Evaluación de la estabilidad del flujo vehicular en las simulaciones.
    \item Evaluación de la reacción del sistema a las fluctuaciones en la demanda.
    \item Verificación de que los patrones emergentes (colas, cuellos de botella, demoras) son congruentes con las conductas anticipadas en situaciones urbanas reales.
\end{itemize}
Aquellos escenarios cuyos resultados simulados se mantuvieron dentro de parámetros lógicos, basados en la literatura y la observación, fueron considerados plausibles.

\subsubsection{Validación comparativa con trabajos y modelos existentes}
Se contrastaron los resultados obtenidos con estudios similares en ciudades latinoamericanas de tamaño y estructura comparable, con el fin de verificar:
\begin{itemize}
    \item Si los niveles de congestión pronosticados son lógicos.
    \item Si los patrones que se han encontrado (saturación en las horas pico, cuellos de botella en vías arteriales) concuerdan con investigaciones anteriores
    \item Si las métricas de centralidad y su interpretación concuerdan con lo que la literatura científica dice acerca de la movilidad urbana.
\end{itemize}
Esta validación facilita el análisis de si la conducta de la red vial en Comayagua se ajusta a patrones típicos en ciudades que están creciendo.

\subsection{Casos de uso}
Finalmente, se desarrollaron casos de uso concretos con el propósito de validar el modelo en diferentes condiciones operativas. Estos escenarios permitieron analizar la 
respuesta del sistema ante variaciones en el flujo vehicular, cambios en la infraestructura y modificaciones en el comportamiento del tránsito. Los casos más relevantes 
fueron los siguientes:\\

\textbf{Caso 1 \- Condición base (hora valle):} \\
Se simuló la operación del tráfico bajo condiciones normales, con un volumen promedio de 300 vehículos por hora. Este escenario fue utilizado para calibrar los parámetros 
iniciales del modelo y establecer una línea de referencia que sirviera como punto de comparación para los demás casos.\\

\textbf{Caso 2 \- Condición de congestión (hora pico):} \\
Se incrementó la demanda vehicular en un 30 \% con respecto al caso base, con el objetivo de analizar el efecto sobre la velocidad promedio y los tiempos de recorrido. Este 
escenario permitió verificar la capacidad del modelo para representar situaciones de saturación y la formación de cuellos de botella dentro de la red vial. \\

\textbf{Caso 3 \- Escenario de contingencia:} \\
Se simuló la sustitución de un semáforo por una rotonda, lo que implicó modificar la estructura del grafo mediante la eliminación de la arista asociada al cruce semaforizado y 
la incorporación de nuevas conexiones que representan el flujo circular. Esta intervención permitió analizar la capacidad del modelo para adaptarse a cambios en la 
infraestructura vial y evaluar cómo se redistribuye el tráfico ante transformaciones físicas como obras, rediseños geométricos o mejoras operativas.\\

